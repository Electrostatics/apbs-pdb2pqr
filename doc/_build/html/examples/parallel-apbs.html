
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parallel APBS execution for large calculations &#8212; APBS-PDB2PQR 3.0 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using the PyMOL APBS plugin" href="using-pymol.html" />
    <link rel="prev" title="Protein-RNA binding linked equilibria" href="salt-linkage.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="parallel-apbs-execution-for-large-calculations">
<h1>Parallel APBS execution for large calculations<a class="headerlink" href="#parallel-apbs-execution-for-large-calculations" title="Permalink to this headline">¶</a></h1>
<div class="section" id="why-parallel">
<h2>Why parallel?<a class="headerlink" href="#why-parallel" title="Permalink to this headline">¶</a></h2>
<p>APBS finite difference multigrid calculations require approximately 200 B memory per grid point.
These memory requirements can be distributed in two ways during a calculation:</p>
<ul class="simple">
<li><p>APBS calculations can be performed in parallel across multiple processors (hopefully, sharing distributed memory!). This functionality is provided by using the <a class="reference internal" href="../apbs/input/elec/mg-para.html#mgpara"><span class="std std-ref">mg-para</span></a> keyword.</p></li>
<li><p>APBS calculations can be broken into a series of smaller, asynchronous runs which (individually) require less memory. This functionality is provided by using both the <a class="reference internal" href="../apbs/input/elec/mg-para.html#mgpara"><span class="std std-ref">mg-para</span></a> and <a class="reference internal" href="../apbs/input/elec/async.html#async"><span class="std std-ref">async</span></a> keywords.</p></li>
</ul>
</div>
<div class="section" id="synchronous-parallel-calculations">
<h2>Synchronous parallel calculations<a class="headerlink" href="#synchronous-parallel-calculations" title="Permalink to this headline">¶</a></h2>
<p>The actin dimer example provided with the APBS distribution <code class="file docutils literal notranslate"><span class="pre">examples/actin-dimer/</span></code> is a fairly large system that can often require too much memory for some systems.
This example will use the actin dimer complex PQR file (<code class="file docutils literal notranslate"><span class="pre">complex.pqr</span></code>) to illustrate parallel focusing.</p>
<p>We’re going to use an 8-processor parallel calculation to write out the electrostatic potential map for this complex.
Each processor will solve a portion of the overall problem using the parallel focusing method on a 973 mesh with 20% overlap between meshes for neighboring processors.
An example input file for this calculation might look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">read</span>
  mol pqr complex.pqr
end
elec name complex
  mg-para
  ofrac <span class="m">0</span>.1
  pdime <span class="m">2</span> <span class="m">2</span> <span class="m">2</span>
  dime <span class="m">97</span> <span class="m">97</span> <span class="m">97</span>
  fglen <span class="m">150</span> <span class="m">115</span> <span class="m">160</span>
  cglen <span class="m">156</span> <span class="m">121</span> <span class="m">162</span>
  cgcent mol <span class="m">1</span>
  fgcent mol <span class="m">1</span>
  mol <span class="m">1</span>
  npbe
  bcfl sdh
  ion <span class="m">1</span> <span class="m">0</span>.150 <span class="m">2</span>.0
  ion -1 <span class="m">0</span>.150 <span class="m">2</span>.0
  pdie <span class="m">2</span>.0
  sdie <span class="m">78</span>.54
  srfm mol
  chgm spl0
  srad <span class="m">1</span>.4
  swin <span class="m">0</span>.3
  sdens <span class="m">10</span>.0
  temp <span class="m">298</span>.15
  calcenergy total
  calcforce no
  write pot dx pot
end
quit
</pre></div>
</div>
<p>where the “<a class="reference internal" href="../apbs/input/elec/pdime.html#pdime"><span class="std std-ref">pdime</span></a> 2 2 2” statement specifies the 8-processor array dimensions, the “<a class="reference internal" href="../apbs/input/elec/ofrac.html#ofrac"><span class="std std-ref">ofrac</span></a> 0.1” statement specifies the 20% overlap between processor calculations, and the “<a class="reference internal" href="../apbs/input/elec/dime.html#dime"><span class="std std-ref">dime</span></a> 97 97 97` statement specifies the size of each processor’s calculation.
The “<a class="reference internal" href="../apbs/input/elec/write.html#write"><span class="std std-ref">write</span></a> pot dx potential” instructs APBS to write out OpenDX-format maps of the potential to 8 files <code class="file docutils literal notranslate"><span class="pre">potential-</span><em><span class="pre">#</span></em><span class="pre">.dx</span></code>, where <em>#</em> is the number of the particular processor.</p>
<p>An MPI-compiled version of APBS can be used with this input file to run 8 parallel focusing calculations, with each calculation generating fine-scale solutions on a different region of the (<a class="reference internal" href="../apbs/input/elec/fglen.html#fglen"><span class="std std-ref">fglen</span></a>) problem domain.
Note that 8 separate OpenDX files are written by the 8 processors used to perform the calculation.
Writing separate OpenDX&lt; files allows us to avoid communication in the parallel run and keeps individual file sizes (relatively) small.
Additionally, if a user is interested in a specific portion of the problem domain, only a few files are needed to get local potential information.
However, most users are interested in global potentials.
APBS provides the <a class="reference internal" href="../apbs/utilities/mergedx.html#mergedx"><span class="std std-ref">mergedx and mergedx2</span></a> program to reassemble the separate OpenDX files into a single file.
<cite>mergedx</cite> is a simple program that allows users to combine several OpenDX files from a parallel focusing calculation into a single map.
This map can be down-sampled from the original resolution to provide coarser datasets for fast visualization, etc.
For example, the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ mergedx <span class="m">65</span> <span class="m">65</span> <span class="m">65</span> pot0.dx pot1.dx pot2.dx pot3.dx pot4.dx pot5.dx pot6.dx pot7.dx
</pre></div>
</div>
<p>will generate a file <code class="file docutils literal notranslate"><span class="pre">gridmerged.dx</span></code> which has downsampled the much larger dataset contained in the 8 OpenDX files into a 65<sup>3</sup> file which would be suitable for rough visualization.
An example of mergedx output visualization is shown in the attached figure.
Note that downsampling isn’t necessary – and often isn’t desirable for high quality visualization or quantitative analysis.</p>
<img alt="../_images/actin_dimer-iso_trans.jpg" src="../_images/actin_dimer-iso_trans.jpg" />
</div>
<div class="section" id="asynchronous-parallel-calculations">
<h2>Asynchronous parallel calculations<a class="headerlink" href="#asynchronous-parallel-calculations" title="Permalink to this headline">¶</a></h2>
<p>The steps described in the previous section can also be performed for systems or binaries which are not equipped for parallel calculations via MPI.
In particular, you can add the statement “<a class="reference internal" href="../apbs/input/elec/async.html#async"><span class="std std-ref">async</span></a> <em>n</em>” to the ELEC <a class="reference internal" href="../apbs/input/elec/mg-para.html#mgpara"><span class="std std-ref">mg-para</span></a> section of the APBS input file to make the single-processor calculation masquerade as processor <em>n</em> of a parallel calculation.</p>
<p>Scalar maps from asynchronous APBS calculations can be combined using the mergedx program as described above.
Currently, energies and forces from asynchronous APBS calculations need to merged manually (e.g., summed) from the individual asynchronous calculation output.
This can be accomplished by simple shell scripts.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">APBS-PDB2PQR</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">APBS-PDB2PQR overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">How to get the software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-help.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apbs/index.html">APBS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../formats/index.html">File formats</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">APBS-PDB2PQR examples and tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html#apbs-examples">APBS examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#visualizing-results">Visualizing results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../support.html">Support for APBS-PDB2PQR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citing.html">Citing your use of our software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reading.html">Suggested reading for APBS-PDB2PQR</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">APBS-PDB2PQR examples and tutorials</a><ul>
      <li>Previous: <a href="salt-linkage.html" title="previous chapter">Protein-RNA binding linked equilibria</a></li>
      <li>Next: <a href="using-pymol.html" title="next chapter">Using the PyMOL APBS plugin</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;Copyright (c) 2010-2020 Battelle Memorial Institute.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/examples/parallel-apbs.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>